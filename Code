{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Varian.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0ghUr8ZQ1nn",
        "outputId": "5f9ec34a-1d35-40a0-9798-6119bdf6cf48"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.11.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.19)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPQbA6byQol7"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import json\n",
        "import time"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqXjK3y-KaR2"
      },
      "source": [
        "class QuestionAnswer():\n",
        "  \"\"\"The class takes list of questions from the doctor and output answer from based on a given research paper\"\"\"\n",
        "  def __init__(self):\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "    self.model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "\n",
        "  def answer(self, text, question):\n",
        "    '''\n",
        "    Takes in a text as a list research paper and a question\n",
        "    output question, answer from the paper based on the question, score(confidence of value)\n",
        "    '''\n",
        "\n",
        "    inputs = self.tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "\n",
        "    text_tokens = self.tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "    answer_start_scores, answer_end_scores = self.model(**inputs, return_dict = False)\n",
        "      \n",
        "    answer_start_scores =  F.softmax(answer_start_scores, dim = 1)\n",
        "    answer_end_scores = F.softmax(answer_end_scores, dim = 1)\n",
        "\n",
        "    answer_start = torch.argmax(\n",
        "        answer_start_scores\n",
        "    )  # Get the most likely beginning of answer with the argmax of the score\n",
        "      \n",
        "      \n",
        "    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
        "\n",
        "     #normalize score probabily of the start and end\n",
        "    score = (torch.max(answer_start_scores).detach()**2 + torch.max(answer_end_scores).detach()**2)/(answer_start_scores.size(1) * 2)\n",
        "      \n",
        "    answer = self.tokenizer.convert_tokens_to_string(self.tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "    return answer, score"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHrWzHAnvKxM"
      },
      "source": [
        "##Web scraping and making API requests\n",
        "def makeRequest(num_req):\n",
        "  \"\"\"This function takes in the number of request query to make from 'https://ascopubs.org' and scrape the abstract\"\"\"\n",
        "  web_path = f'https://ascopubs.org/action/doSearch?AllField=radiotherapy%2C+follow-up%2C+survivors&target=default&pageSize={num_req}&startPage=0'\n",
        "\n",
        "  res = requests.get(web_path)\n",
        "  soup = BeautifulSoup(res.content, \"html.parser\")\n",
        "  data = soup.find_all(\"a\", class_=\"ref nowrap\")\n",
        "  for d in data:\n",
        "    link = 'https://ascopubs.org' + d['href']\n",
        "    resp = requests.get(link)\n",
        "    soup2 = BeautifulSoup(resp.content, \"html.parser\")\n",
        "    data = soup2.find_all(\"div\", class_=\"NLM_sec NLM_sec_level_1\")\n",
        "    content = []\n",
        "    for about in data:\n",
        "      \n",
        "      try:\n",
        "        headings = about.find(\"div\", class_=\"sectionHeading\")\n",
        "        paragraph = about.find(\"p\")\n",
        "        if len(paragraph.text.split(' ')) < 512:\n",
        "          content.append(paragraph.text)\n",
        "      except:\n",
        "        print('Just minor error')\n",
        "    yield content\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i41Y_7z5IklI"
      },
      "source": [
        "#9739161f3bea474f9bd7edfc8a2c1b38\n",
        "def analyzeMedicalDoc(documents):\n",
        "  \"\"\" This function make request at azure /text/analytics/v3.1/entities/health/jobs endpoint\"\"\"\n",
        "  '''Provide your Api Key'''\n",
        "  Ocp_Apim_Subscription_Key = 'xxxxxxxxxxxxxxxxxxxxxxxxxxxx'\n",
        "  header = {\"Ocp-Apim-Subscription-Key\": Ocp_Apim_Subscription_Key, \"Content-Type\": \"application/json\", \"Accept\":\"application/json\"}\n",
        "  doc_file = []\n",
        "  for i,doc in enumerate(documents):\n",
        "    doc = {\"language\": \"en\", \"id\": f\"{i+1}\", \"text\":f\"{doc}\"}\n",
        "    doc_file.append(doc)\n",
        "  \n",
        "  \n",
        "  param ={\n",
        "    \"documents\": doc_file\n",
        "  }\n",
        "\n",
        "  param = json.dumps(param, indent = 4)\n",
        "\n",
        "  max_tries = 0\n",
        "  \n",
        "  resp = requests.post('https://jachanya.cognitiveservices.azure.com/text/analytics/v3.1/entities/health/jobs', headers = header, data = param)\n",
        "  \n",
        "  \n",
        "\n",
        "  \"\"\"get the response from the post\"\"\"\n",
        "  jobId = resp.headers['apim-request-id']\n",
        "  operation_location = resp.headers['operation-location']\n",
        "\n",
        "  params = {\"jobId\":f\"{jobId}\"}\n",
        "  headers = {\"Ocp-Apim-Subscription-Key\": Ocp_Apim_Subscription_Key, \"Content-Type\": \"application/json\"}\n",
        "  max_tries = 0\n",
        "  while max_tries < 20:\n",
        "\n",
        "    res = requests.get(f'{operation_location}', headers = headers, params = params)\n",
        "    if res.json()['status'] == 'succeeded':\n",
        "      break\n",
        "    max_tries +=1\n",
        "  \n",
        "  is_delete = requests.delete(f'{operation_location}', headers = headers, params = params)\n",
        "  return res.json()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKATiG2yQwJw"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPk_IBTXpKOt"
      },
      "source": [
        "def getSymDiagTr(documents):\n",
        "  \"\"\"Get symptoms, diagnosis, treatment of a paper\"\"\"\n",
        "  \n",
        "  #print(json.dumps(resp, indent = 4))\n",
        "  try:\n",
        "    resp = analyzeMedicalDoc(documents)\n",
        "    if resp['status'] == 'succeeded':\n",
        "      symptomsnsigns = set()\n",
        "      treatmentname = set()\n",
        "      diagnosis = set()\n",
        "      for x in resp[\"results\"][\"documents\"]:\n",
        "        for i,entity in enumerate(x[\"entities\"]):\n",
        "          \n",
        "          if entity['category'] == 'Diagnosis':\n",
        "            \n",
        "            if entity.get('name', \"\") != \"\":\n",
        "              diagnosis.add(entity.get('name', \"\"))\n",
        "            else:\n",
        "              diagnosis.add(entity['text'])\n",
        "\n",
        "          \n",
        "          if entity['category'] == 'SymptomOrSign':\n",
        "            \n",
        "            if entity.get('name', \"\") != \"\":\n",
        "              symptomsnsigns.add(entity.get('name', \"\"))\n",
        "            else:\n",
        "              symptomsnsigns.add(entity['text'])\n",
        "\n",
        "          \n",
        "          if entity['category'] == 'TreatmentName':\n",
        "            \n",
        "            if entity.get('name', \"\") != \"\":\n",
        "              treatmentname.add(entity.get('name', \"\"))\n",
        "            else:\n",
        "              treatmentname.add(entity['text'])\n",
        "      return symptomsnsigns,diagnosis,treatmentname\n",
        "  except:\n",
        "    print('Some random error')\n",
        "  return None"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-87YjqYQqgJ"
      },
      "source": [
        "researcher = QuestionAnswer()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7BHcegzRVT2"
      },
      "source": [
        "def find_similarity(patient, nbr_papers):\n",
        "  \"\"\"This function find similarity between patients and different papers, we select the most corroleted\"\"\"\n",
        "  current_paper = 'xxx'\n",
        "  similarity = torch.zeros(3, nbr_papers)\n",
        "  counter = 0\n",
        "  max_x = 0\n",
        "  max_y = 0\n",
        "  max_z = 0\n",
        "  for texts in makeRequest(nbr_papers):\n",
        "    if len(texts) != 0:\n",
        "      try:\n",
        "        result = getSymDiagTr(texts)\n",
        "        if result is not None:\n",
        "          \n",
        "          sym, diag, treat = result\n",
        "          #print(len(set(patient.symptoms).intersection(sym)))\n",
        "\n",
        "          if len(sym) > max_x: max_x = len(sym)\n",
        "          if len(diag) > max_y: max_y = len(diag)\n",
        "          if len(treat) > max_z: max_z = len(treat)\n",
        "\n",
        "\n",
        "          similarity[0, counter] = len(set(patient.symptoms).intersection(sym)) * len(sym)\n",
        "          similarity[1, counter] = len(set(patient.diagnosis).intersection(diag)) * len(diag)\n",
        "          similarity[2, counter] = len(set(patient.treatment).intersection(treat)) * len(treat)\n",
        "      \n",
        "      except:\n",
        "        raise\n",
        "      counter = counter + 1\n",
        "\n",
        "    similarity[0, :] /= max_x\n",
        "    similarity[1, :] /= max_y\n",
        "    similarity[2, :] /= max_z\n",
        "\n",
        "  return torch.argmax(torch.sum(similarity, dim = 0))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88zX4HC2ckmw"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DNfrnNOR1eE"
      },
      "source": [
        "class Patient():\n",
        "  \"\"\" A hypothetical patient class\"\"\"\n",
        "  def __init__(self,name, id):\n",
        "    self.symptoms = []\n",
        "    self.diagnosis = []\n",
        "    self.treatment = []\n",
        "  \n",
        "  def add_symptoms(self, symp):\n",
        "    self.symptoms.append(symp)\n",
        "  \n",
        "  def add_diagnosis(self, diagnosis):\n",
        "    self.diagnosis.append(diagnosis)\n",
        "\n",
        "  def add_treatment(self, treatment):\n",
        "    self.treatment.append(treatment)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULZPGhTKR88q"
      },
      "source": [
        "name = 'Musa'\n",
        "id = '1029r387ufhwks-33rt'\n",
        "\n",
        "#Fake patient and patient data\n",
        "pa = Patient(name, id)\n",
        "pa.add_diagnosis('Cessation of life')\n",
        "pa.add_symptoms('risk factors')\n",
        "pa.add_treatment('Therapeutic radiology procedure')\n",
        "max_sc = find_similarity(pa,30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvTi845Lo3U4"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIVhtdkDuUAA"
      },
      "source": [
        "def getMostRelatedPaper(num_req):\n",
        "  \n",
        "  web_path = f'https://ascopubs.org/action/doSearch?AllField=radiotherapy%2C+follow-up%2C+survivors&target=default&pageSize={num_req}&startPage=0'\n",
        "\n",
        "  link = ''\n",
        "  res = requests.get(web_path)\n",
        "  soup = BeautifulSoup(res.content, \"html.parser\")\n",
        "  data = soup.find_all(\"a\", class_=\"ref nowrap\")\n",
        "  for d in data:\n",
        "    link = 'https://ascopubs.org' + d['href']\n",
        "  resp = requests.get(link)\n",
        "  soup2 = BeautifulSoup(resp.content, \"html.parser\")\n",
        "  data = soup2.find_all(\"div\", class_=\"NLM_sec NLM_sec_level_1\")\n",
        "  content = []\n",
        "  for about in data:\n",
        "      \n",
        "    try:\n",
        "      headings = about.find(\"div\", class_=\"sectionHeading\")\n",
        "      paragraph = about.find(\"p\")\n",
        "      if len(paragraph.text.split(' ')) < 512:\n",
        "        content.append(paragraph.text)\n",
        "    except:\n",
        "      print('Just minor error')\n",
        "  return content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxgRmeELUDEs"
      },
      "source": [
        "def docQueryPaper(questions, max_sc):\n",
        "  '''Function for doctor to query papers based on contents from most similar paper to patient'''\n",
        "  qa = QuestionAnswer()\n",
        "  contents = getMostRelatedPaper(max_sc)\n",
        "  for question in questions:\n",
        "    for content in contents:\n",
        "      answer, score = qa.answer(content, question)\n",
        "      print(question+ ' ' +answer)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkDWXdvYw5Es"
      },
      "source": [
        "docQueryPaper(['what is this paper about?'], max_sc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk9SLPSTxR3t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
